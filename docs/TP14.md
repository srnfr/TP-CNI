# Hubble

Ce TP se déroule sur un cluster <ins>**KinD**<ins> (bien que DigitalOcean le supporte maintenant). 

## Sommaire
  * [But du TP](#but-du-tp)
  * [Pre-requis](#pre-requis)
  * [Hubble CLI](#hubble-cli)
  * [Hubble UI](#hubble-ui)
  * [Hubble observe sur un Node](#hubble-observe-sur-un-node)
  * [Hubble Relay](#hubble-relay)
  * [Jouez !](#jouez-)
  * [Cleanup](#cleanup)

## But du TP
Savoir installer, accéder et utiliser Hubble en UI et en CLI 

## Pre-requis

NB: Si vous souhaitez utiliser votre *cluster DigitalOcean*, Hubble est déjà activé, passez directement à [l'étape suivante](https://github.com/srnfr/TP-CNI/blob/main/docs/TP14.md#hubble-ui).

*Sur la VM* dont les accès vous ont été fournis par l'animateur :

Nettoyons les eventuels clusters déjà présents :
```shell
cd /home/cilium_lab/basic
./clean-kind.sh 
```

Lançons un cluster *sans* CNI :
```shell
./01-install-cluster.sh
```

Installons le CNI
```shell
helm upgrade --install --namespace kube-system --repo https://helm.cilium.io cilium cilium --version 1.15.1 --values ebpf-values-nokubeproxy.yaml
```

## Hubble CLI

A l'instar de `cilium`, il existe un CLI pour `hubble` ([lien](https://docs.cilium.io/en/stable/gettingstarted/hubble_setup/#install-the-hubble-client)), mais il est **déjà installé** sur votre VM ou votre GitPod/CodeSpaces :

```shell
# hubble version
hubble 0.12.0 compiled with go1.20.5 on linux/amd64
```

Au bout de plusieurs minutes, vérifier que Cilium est ok :
```shell
cilium status
```
Lancer les tests de connectivité :
```shell
cilium connectivity  test
```

Huble ne peut être installé en même temps en CLI et en Helm.
Si on fait en Helm, il faut s'assurer que le paragraphe est présent dans le fichier ebpf-values.yaml précisé dans la commande Helm:
```yaml
hubble:
  enabled: true
  relay:
    enabled: true
  ui:
    enabled: true
```

## Hubble UI

On lance ensuite Hubble UI :
```bash
cilium hubble ui &
```

L'UI écoute sur le port TCP/12000 :
```
# ℹ️  Opening "http://localhost:12000" in your browser...
```

Reperez l'@IP publique de la VM :
```shell
# ifconfig eth0
eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet **46.101.186.zzz**  netmask 255.255.192.0  broadcast 46.101.191.255
        inet6 fe80::47a:17ff:fea2:b9e9  prefixlen 64  scopeid 0x20<link>
        ether 06:7a:17:a2:b9:e9  txqueuelen 1000  (Ethernet)
        RX packets 85068  bytes 1689421494 (1.6 GB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 31382  bytes 5277651 (5.2 MB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
```

Connectez-vous sur http://46.101.186.zzz:12001 (un nginx local reverse-proxyfie 12001 vers localhst:12000)

Pour voir du traffic, lancez un `cilium connectivity test` et observez le ns `cilium-test`.

*PS* : il est plus élégant de faire :
```shell
kubectl port-forward -n kube-system svc/hubble-ui 12000:80
```

## `Hubble Observe` sur un Node

On peut obtenir des informations directement en ligne de commande sur un des Pod Cilium :

```shell
kubectl exec -it ds/cilium -n kube-system -c cilium-agent -- hubble observe           
```

Il existe de nombreuses options de filtrage.   
Obtenez en une liste par un `hubble help observe`.

## Hubble Relay

Pour avoir des informations aggrégées de tous les noeuds, il faut s'addresser au Hubble Relay qui collecte les informations de tous les serveurs Hubble embarqués dans les cilium-agent.

```shell
kubectl -n kube-system port-forward service/hubble-relay --address 0.0.0.0 --address :: 4245:80 &
```

Depuis notre VM on peut requeter :
```shell
hubble status
```

On obtient :
```
Handling connection for 4245
Healthcheck (via localhost:4245): Ok
Current/Max Flows: 880/16,380 (5.37%)
Flows/s: 7.04
Connected Nodes: 4/4
```

```shell
# hubble list nodes
Handling connection for 4245
NAME                  STATUS      AGE     FLOWS/S   CURRENT/MAX-FLOWS
basic-control-plane   Connected   2m29s   3.21      510/4095 ( 12.45%)
basic-worker          Connected   2m28s   0.59      106/4095 (  2.59%)
basic-worker2         Connected   2m32s   0.71      134/4095 (  3.27%)
basic-worker3         Connected   2m32s   2.61      428/4095 ( 10.45%)
```

On peut analyser les flux :
```shell
# hubble observe -f
Handling connection for 4245
Sep 14 05:44:59.479: 172.18.0.5:54612 (remote-node) <- 10.244.3.249:4240 (health) to-stack FORWARDED (TCP Flags: ACK)
Sep 14 05:44:59.481: 172.18.0.4:39792 (remote-node) -> 10.244.3.249:4240 (health) to-endpoint FORWARDED (TCP Flags: ACK, PSH)
Sep 14 05:44:59.481: 172.18.0.4:39792 (remote-node) <- 10.244.3.249:4240 (health) to-stack FORWARDED (TCP Flags: ACK)
Sep 14 05:44:59.481: 172.18.0.4 (remote-node) -> 10.244.3.249 (health) to-endpoint FORWARDED (ICMPv4 EchoRequest)
Sep 14 05:44:59.481: 172.18.0.4 (remote-node) <- 10.244.3.249 (health) to-stack FORWARDED (ICMPv4 EchoReply)
```

Vous pouvez ajouter le paramètre `-n default` et/ou `--verdict DROPPED` pour vous restreindre à un namespace.

## Jouez !!

Déployez à nouveau un TP de filtrage précedent et observez les flux bloqués ou ceux qui passent...
Si vous n'avez pas d'idée :

```shell
cilium connectivity test
```

Pour une courte démo de ce que l'on peut faire avec Hubble, vous pouvez regarder cette vidéo [lien](https://www.youtube.com/watch?v=zQr4tEj-a4M)

## Cleanup

Effaçons notre cluster :

```shell
cd /home/cilium_lab/basic
./clean-kind.sh 
```
---



[Revenir au sommaire](../README.md) | [TP Suivant](./TP14.md)
