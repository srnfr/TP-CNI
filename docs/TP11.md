# Preservation de l'IP source avec `externalTrafficPolicy`

Ce TP se déroule sur un cluster <ins>**DigitalOcean**<ins>.  

## Sommaire
  * [But du TP](#but-du-tp)
  * [Deployer l'application Guestbook](#deployer-lapplication-guestbook)
  * [Depuis l'interieur du Cluster](#depuis-linterieur-du-cluster)
  * [Depuis l'exterieur](#depuis-lexterieur)
  * [Modifier le `externalTrafficPolicy](#modifier-le-externaltrafficpolicy)
  * [Cleanup](#cleanup)

## But du TP
Comprendre comment les @IP exterieures sont modifiées (i.e SNAT) avant d'arriver au niveau des Pod serveur.
Montrer comment `externalTrafficPolicy` permet de modifier ce comportement.

## Deployer l'application Guestbook

On utilise l'app Guestbook vu précedemment, i.e [TP04](./TP04.md) :
```bash
kubectl apply -f frontend-deployment.yaml
kubectl apply -f frontend-service.yaml
```

Configurer un replicaset du frontend à 1 pour plus de simplicité.
```bash
kubectl scale deployment frontend --replicas=1
```

## Depuis l'interieur du Cluster

Dans une fenêtre Terminal de , observez les logs de l'application pour identifier l'IP source

```bash
kubectl logs -f frontend-xxxx 
```

Dans un autre fenêtre terminal, depuis le Pod `debug-blue` (cf [TP05](./TP05.md)), tester l'acces au svc `frontend` de l'application Guestbook avec `curl` par exemple :

```bash
A vous de trouver la commande...
```

Dans la fenêtre de log, vous devriez obtenir quelque chose comme ceci :
```
10.244.0.202 - - [14/Sep/2022:19:31:28 +0000] "GET / HTTP/1.1" 200 1172 "-" "curl/7.83.1"
10.244.0.202 - - [14/Sep/2022:19:31:48 +0000] "GET / HTTP/1.1" 200 1172 "-" "curl/7.83.1"
```
Vérifier que c'est bien l'IP du pod `debug-blue` et que donc l'IP est conservée en intra-cluster.

## Depuis l'exterieur

Depuis l'extérieur (votre PC ou CodeSpaces), consultez le site plusieurs fois en pointant sur l'IP du service frontend.

Vous constatez dans les logs que l'IP vue depuis le frontend est changeante.
```
10.135.152.70 - - [14/Sep/2022:19:36:30 +0000] "GET / HTTP/1.1" 200 1172 "-" "curl/7.79.1"
164.92.138.123 - - [14/Sep/2022:19:36:40 +0000] "GET / HTTP/1.1" 200 1172 "-" "curl/7.79.1"
10.135.152.70 - - [14/Sep/2022:19:36:45 +0000] "GET / HTTP/1.1" 200 1172 "-" "curl/7.79.1"
10.135.152.70 - - [14/Sep/2022:19:36:49 +0000] "GET / HTTP/1.1" 200 1172 "-" "curl/7.79.1"
```

Pourquoi ?

## Modifier le `externalTrafficPolicy`

Modifier le service d'exposition du Frontend afin que le champ `externalTrafficPolicy` soit à `Local` (et non `Cluster` comme par défaut) :

```bash
    kubectl patch svc frontend -p '{"spec":{"externalTrafficPolicy":"Local"}}'
```

De nouveau, consultez le site depuis l'extérieur, et constatez que l'IP source est maintenant stable. En théorie elle devrait correspondre  à l'IP publique du visiteur, mais [l'implementation spécifique de DigitalOcean](https://docs.digitalocean.com/products/kubernetes/how-to/configure-load-balancers/#external-traffic-policies-and-health-checks) ne le permet pas et utilise une IP privée fixe affectée au LB.

Dans la description du service frontend, déterminer quel est le port TCP (`HealthCheck NodePort`)qui permet de publier si un Pod est présent sur un Noeud.

Depuis CodeSpaces (puisque votre PC est probablement filtré) faites des requetes sur ce port sur les différentes et observez la page servie.
Vous devriez obtenir quelque chose comme ceci :

```
% curl <NODE1_PUBLIC_IP>:<HEALTHCHECK_PORT>
{
        "service": {
                "namespace": "default",
                "name": "frontend"
        },
        "localEndpoints": 0
}%                              
```
et
```
curl <NODE2_PUBLIC_IP>:<HEALTHCHECK_PORT>
{
        "service": {
                "namespace": "default",
                "name": "frontend"
        },
        "localEndpoints": 1
}%                                 
```
Vérifier que l'unique Pod frontend se trouve sur le Node pressenti.

Augmentez le nombre de replicas du frontend à 3 (```kubectl scale deploy/frontend --replicas=3```), et constatez que l'IP source vue est toujours stable mais que la répartition n'est pas de 1/2 sur chaque noeud, mais (probablement) 2/3 sur un noeud et 1/3 sur l'autre.

## Cleanup

Détruire le déploiement et le svc :
```bash
kubectl delete deploy/frontend
kubectl delete svc/frontend
```

---

[Revenir au sommaire](../README.md) | [TP Suivant](./TP12.md)
